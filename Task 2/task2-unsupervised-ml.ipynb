{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Author: Abhilash Pandey"},{"metadata":{},"cell_type":"markdown","source":"**Data Science & Business Analytics Intern (Batch-Dec'20)**"},{"metadata":{},"cell_type":"markdown","source":"**TASK 2**"},{"metadata":{},"cell_type":"markdown","source":"Prediction using Unsupervised ML"},{"metadata":{},"cell_type":"markdown","source":"**K-Means Clustering**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Importing required Libraries\n\nimport pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nfrom sklearn import datasets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Loading the required dataset\n\niris=pd.read_csv('../input/iris/Iris.csv')\niris.drop([\"Id\"],axis=1,inplace=True)\niris.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Viewing the statistical Info\niris.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the present datatypes\niris.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**For proper visualization of data points distribution**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data=iris,hue=\"Species\",palette=\"Set1\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nFrom above visuals iris-setosa is easily separable from the other two."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nfeatures = iris.loc[:,[\"SepalLengthCm\",\"SepalWidthCm\",\"PetalLengthCm\",\"PetalWidthCm\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Below snippet shows how we can find the optimum number of clusters for K Means and how can we determine the value of K?**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding the optimum number of clusters for k-means classification\n\nx = iris.iloc[:, [0, 1, 2, 3]].values\n\nfrom sklearn.cluster import KMeans\nwcss = []\n\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', \n                    max_iter = 300, n_init = 10, random_state = 0)\n    kmeans.fit(x)\n    wcss.append(kmeans.inertia_)\n    \n# Plotting the results onto a line graph, \n# `allowing us to observe 'The elbow'\nplt.plot(range(1, 11), wcss)\nplt.title('The elbow method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS') # Within cluster sum of squares\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this we choose the number of clusters as 3."},{"metadata":{},"cell_type":"markdown","source":"**Verifying visually that with which cluster number, K-means will be optimum**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(24,4))\n\nplt.suptitle(\"K Means Clustering\",fontsize=20)\n\n\nplt.subplot(1,5,1)\nplt.title(\"K = 1\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nplt.ylabel(\"PetalWidthCm\")\nplt.scatter(features.PetalLengthCm,features.PetalWidthCm)\n\n\nplt.subplot(1,5,2)\nplt.title(\"K = 2\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nkmeans = KMeans(n_clusters=2)\nfeatures[\"labels\"] = kmeans.fit_predict(features)\nplt.scatter(features.PetalLengthCm[features.labels == 0],features.PetalWidthCm[features.labels == 0])\nplt.scatter(features.PetalLengthCm[features.labels == 1],features.PetalWidthCm[features.labels == 1])\n\n# dropping labels we only want to use features.\nfeatures.drop([\"labels\"],axis=1,inplace=True)\n\nplt.subplot(1,5,4)\nplt.title(\"K = 3\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nkmeans = KMeans(n_clusters=3)\nfeatures[\"labels\"] = kmeans.fit_predict(features)\nplt.scatter(features.PetalLengthCm[features.labels == 0],features.PetalWidthCm[features.labels == 0])\nplt.scatter(features.PetalLengthCm[features.labels == 1],features.PetalWidthCm[features.labels == 1])\nplt.scatter(features.PetalLengthCm[features.labels == 2],features.PetalWidthCm[features.labels == 2])\n\n# dropping labels as we only want to use features.\nfeatures.drop([\"labels\"],axis=1,inplace=True)\n\nplt.subplot(1,5,3)\nplt.title(\"K = 4\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nkmeans = KMeans(n_clusters=4)\nfeatures[\"labels\"] = kmeans.fit_predict(features)\nplt.scatter(features.PetalLengthCm[features.labels == 0],features.PetalWidthCm[features.labels == 0])\nplt.scatter(features.PetalLengthCm[features.labels == 1],features.PetalWidthCm[features.labels == 1])\nplt.scatter(features.PetalLengthCm[features.labels == 2],features.PetalWidthCm[features.labels == 2])\nplt.scatter(features.PetalLengthCm[features.labels == 3],features.PetalWidthCm[features.labels == 3])\n\n# dropping labels as we only want to use features.\nfeatures.drop([\"labels\"],axis=1,inplace=True)\n\nplt.subplot(1,5,5)\nplt.title(\"Original Labels\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nplt.scatter(iris.PetalLengthCm[iris.Species == \"Iris-setosa\"],iris.PetalWidthCm[iris.Species == \"Iris-setosa\"])\nplt.scatter(iris.PetalLengthCm[iris.Species == \"Iris-versicolor\"],iris.PetalWidthCm[iris.Species == \"Iris-versicolor\"])\nplt.scatter(iris.PetalLengthCm[iris.Species == \"Iris-virginica\"],iris.PetalWidthCm[iris.Species == \"Iris-virginica\"])\n\nplt.subplots_adjust(top=0.8)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying kmeans to the dataset / Creating the kmeans classifier\nkmeans = KMeans(n_clusters = 3, init = 'k-means++',\n                max_iter = 300, n_init = 10, random_state = 0)\ny_kmeans = kmeans.fit_predict(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualising the clusters - On the first two columns\nplt.scatter(x[y_kmeans == 0, 0], x[y_kmeans == 0, 1], \n            s = 100, c = 'red', label = 'Iris-setosa')\nplt.scatter(x[y_kmeans == 1, 0], x[y_kmeans == 1, 1], \n            s = 100, c = 'blue', label = 'Iris-versicolour')\nplt.scatter(x[y_kmeans == 2, 0], x[y_kmeans == 2, 1],\n            s = 100, c = 'green', label = 'Iris-virginica')\n\n# Plotting the centroids of the clusters\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], \n            s = 100, c = 'yellow', label = 'Centroids')\n\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Hence the optimum number of clusters are predicted and displayed.**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}